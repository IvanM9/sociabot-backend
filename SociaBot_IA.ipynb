{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvanM9/sociabot-backend/blob/moreira/SociaBot_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **SociaBot IA** 🤖"
      ],
      "metadata": {
        "id": "mwQU46EMXJtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalar llama-cpp-python y dependencias**"
      ],
      "metadata": {
        "id": "O4pQ_-OxUcTx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a-CQO8LJVGn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e28e316-ed68-45bb-ff36-eda03a78a7d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.37.tar.gz (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.4)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.37-cp310-cp310-manylinux_2_35_x86_64.whl size=2513487 sha256=c536a33c8e17368001eacb65117f7dcec84a8fea09f1d12b38947666954767b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/4f/36/1df917d3d21e5d04ce960616a6fb9e39c1548149620528a67c\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.37\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencias necesarias"
      ],
      "metadata": {
        "id": "5d45g4N0Ugi1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "znldebdFV6kc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d3e69d-4cac-458b-a0e6-504a96fcd852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere\n",
            "  Downloading cohere-4.45-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.1)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Installing collected packages: kaleido, typing-extensions, python-multipart, importlib_metadata, h11, fastavro, backoff, tiktoken, httpcore, httpx, cohere, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.45 fastavro-1.9.3 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 importlib_metadata-6.11.0 kaleido-0.2.1 openai-1.10.0 python-multipart-0.0.6 tiktoken-0.5.2 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kaleido python-multipart openai tiktoken cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalar servidor para los modelos"
      ],
      "metadata": {
        "id": "eOsnvgVRUi6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yddFiH1mVV09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f475614f-54a2-4174-a57b-58ad7197afe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python[server] in /usr/local/lib/python3.10/dist-packages (0.2.37)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (4.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (3.1.3)\n",
            "Collecting uvicorn>=0.22.0 (from llama-cpp-python[server])\n",
            "  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.100.0 (from llama-cpp-python[server])\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-settings>=2.0.1 (from llama-cpp-python[server])\n",
            "  Downloading pydantic_settings-2.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting sse-starlette>=1.6.1 (from llama-cpp-python[server])\n",
            "  Downloading sse_starlette-2.0.0-py3-none-any.whl (9.0 kB)\n",
            "Collecting starlette-context<0.4,>=0.3.6 (from llama-cpp-python[server])\n",
            "  Downloading starlette_context-0.3.6-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.100.0->llama-cpp-python[server]) (1.10.14)\n",
            "Collecting starlette<0.36.0,>=0.35.0 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python[server]) (2.1.4)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.1->llama-cpp-python[server])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette>=1.6.1->llama-cpp-python[server]) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.22.0->llama-cpp-python[server]) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.22.0->llama-cpp-python[server]) (0.14.0)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.16.1 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100.0->llama-cpp-python[server])\n",
            "  Downloading pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette>=1.6.1->llama-cpp-python[server]) (1.2.0)\n",
            "Installing collected packages: uvicorn, python-dotenv, pydantic-core, annotated-types, starlette, pydantic, starlette-context, sse-starlette, pydantic-settings, fastapi\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.14\n",
            "    Uninstalling pydantic-1.10.14:\n",
            "      Successfully uninstalled pydantic-1.10.14\n",
            "Successfully installed annotated-types-0.6.0 fastapi-0.109.0 pydantic-2.6.0 pydantic-core-2.16.1 pydantic-settings-2.1.0 python-dotenv-1.0.1 sse-starlette-2.0.0 starlette-0.35.1 starlette-context-0.3.6 uvicorn-0.27.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python[server]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos para instalar**"
      ],
      "metadata": {
        "id": "iiKTjaKrVNQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo LLAMA 2 7b Q5_0"
      ],
      "metadata": {
        "id": "XOe6UD_-Tt5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf"
      ],
      "metadata": {
        "id": "4SHTSSysTwku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mistral-7b-instruct-v0.1.Q4_K_M"
      ],
      "metadata": {
        "id": "0I4Amg2eT_cQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6lpFXrZeVgsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0232b5d4-27ef-4e7a-8f71-4579bd8c0004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-31 14:11:21--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.118, 65.8.178.27, 65.8.178.93, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1706969481&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjk2OTQ4MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=EDHtzoLWD6e4qcHLrQuQbjPsa37uqZZpPmEEvOtM19mMx7Z7KFEpPwClfe8cZN9YPbqnPihwxIJZIICtXHKhHV1NSJWONvhHvJkaPbikqfemPb1rl1OK3IHo8p2yt3cJK9wBj9AsoLNbBx1vJPwQajiGxe2rwAq6Uu4ewSK-Tgtb1HH9pPVEadShtLWfl6tJUe80rkOgXuZJOCT%7EF3D%7EzKg2DvagUyfoZEgZBufaC3NYWIb5tF6qB6OgAZI7C01IXkN43PVEPJ4AfnZ9GGGNnraaBIAwIX5IVRiuZirVmuD6SdPDOfhAYGTRFci2gI45x6LBPXofVKKWBUORFbdfUw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-01-31 14:11:21--  https://cdn-lfs.huggingface.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1706969481&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjk2OTQ4MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=EDHtzoLWD6e4qcHLrQuQbjPsa37uqZZpPmEEvOtM19mMx7Z7KFEpPwClfe8cZN9YPbqnPihwxIJZIICtXHKhHV1NSJWONvhHvJkaPbikqfemPb1rl1OK3IHo8p2yt3cJK9wBj9AsoLNbBx1vJPwQajiGxe2rwAq6Uu4ewSK-Tgtb1HH9pPVEadShtLWfl6tJUe80rkOgXuZJOCT%7EF3D%7EzKg2DvagUyfoZEgZBufaC3NYWIb5tF6qB6OgAZI7C01IXkN43PVEPJ4AfnZ9GGGNnraaBIAwIX5IVRiuZirVmuD6SdPDOfhAYGTRFci2gI45x6LBPXofVKKWBUORFbdfUw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.162.99, 108.157.162.58, 108.157.162.95, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.162.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368438944 (4.1G) [binary/octet-stream]\n",
            "Saving to: ‘mistral-7b-instruct-v0.1.Q4_K_M.gguf’\n",
            "\n",
            "mistral-7b-instruct 100%[===================>]   4.07G  48.9MB/s    in 92s     \n",
            "\n",
            "2024-01-31 14:12:54 (45.4 MB/s) - ‘mistral-7b-instruct-v0.1.Q4_K_M.gguf’ saved [4368438944/4368438944]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dolphin-2.6-mistral-7b.Q3_K_S"
      ],
      "metadata": {
        "id": "kNAH2ZDBUHC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b.Q3_K_S.gguf"
      ],
      "metadata": {
        "id": "0fV6J6g8sicv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama2-chat-ayb-13b.Q4_K_M"
      ],
      "metadata": {
        "id": "ndekLAuiULsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama2-chat-AYB-13B-GGUF/resolve/main/llama2-chat-ayb-13b.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "pSaS3fp5A2Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "firefly-llama2-13b-chat.Q3_K_M"
      ],
      "metadata": {
        "id": "J7jlNp1sURbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/firefly-llama2-13B-chat-GGUF/resolve/main/firefly-llama2-13b-chat.Q3_K_M.gguf"
      ],
      "metadata": {
        "id": "7yp8x4Lql5jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "toxicqa-llama2-7b.Q2_K"
      ],
      "metadata": {
        "id": "zn0lnQshUVz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/toxicqa-Llama2-7B-GGUF/resolve/main/toxicqa-llama2-7b.Q2_K.gguf"
      ],
      "metadata": {
        "id": "pZLECMarursU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "neural-chat-7b-v3-1.Q3_K_S"
      ],
      "metadata": {
        "id": "97b_Oe1iUYOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/neural-chat-7B-v3-1-GGUF/resolve/main/neural-chat-7b-v3-1.Q3_K_S.gguf"
      ],
      "metadata": {
        "id": "gYQ0CFtUz23-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Probar Modelo**"
      ],
      "metadata": {
        "id": "7hw-cywfVW7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# En model_path debe ingresarse el nombre del modelo instalado\n",
        "```"
      ],
      "metadata": {
        "id": "rxWrF2bRUpQ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MOYHvzR3X4kT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22b0019-f5c7-45ed-9dc8-1b4298e6243c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-55ec06dd-03c4-47cb-8522-f874b6c76723', 'object': 'text_completion', 'created': 1706710374, 'model': 'mistral-7b-instruct-v0.1.Q4_K_M.gguf', 'choices': [{'text': 'Q: Para que sirve el cerebro? A: 1.3640273097 - 12', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 17, 'total_tokens': 31}}\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\")\n",
        "output = llm(\"Q: Para que sirve el cerebro? A: \", max_tokens=4092, stop=[\"Q:\", \"\\n\"], echo=True)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuracion de Ngrok**"
      ],
      "metadata": {
        "id": "dTLVBFBWViIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalacion de ngrok"
      ],
      "metadata": {
        "id": "50dmAerkVl7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SbV0cI4_Ygio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f0efef-39b7-4ce1-fbfd-33b325a67ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-31 14:13:17--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 52.202.168.65, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  57.1MB/s    in 0.2s    \n",
            "\n",
            "2024-01-31 14:13:17 (57.1 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usar token de ngrok"
      ],
      "metadata": {
        "id": "3ZKsBJaqV6mD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LCm1VVPvYqWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d411483b-9095-4943-a4fc-ebedb6485a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "token = userdata.get('AUTHGROK_N')\n",
        "!./ngrok authtoken 23DfsO3h97yUPTQOqilsFDi4IcE_4J681vGPqAViS5As9f5EW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejecucion de Servidor**"
      ],
      "metadata": {
        "id": "jbgrm0xfWEyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se ejecuta el server\n",
        "```\n",
        "# --model debe ser reemplazado por el modelo descargado\n",
        "```"
      ],
      "metadata": {
        "id": "UOCMOf7SWIm8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5pH7dWnOYuWh"
      },
      "outputs": [],
      "source": [
        "!python3 -m llama_cpp.server --model mistral-7b-instruct-v0.1.Q4_K_M.gguf --host 127.0.0.1 > server.log 2>&1 &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se redirecciona el server"
      ],
      "metadata": {
        "id": "cj4ASJX_WTNT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HWa6ZX1VZDtE"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().system_raw('./ngrok http 8000 &')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtencion de url para ver ejecucion\n",
        "```\n",
        "# Se agrega /docs al final de la URL\n",
        "```"
      ],
      "metadata": {
        "id": "bMBfSpR7WbqB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i4u1y3OWZJ70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1148dac3-65bf-4053-ce0d-7caa25029504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ]
        }
      ],
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Detener Server**"
      ],
      "metadata": {
        "id": "XkKRC932W0RT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matar servicio"
      ],
      "metadata": {
        "id": "DrLoS-pwW4tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill uvicorn"
      ],
      "metadata": {
        "id": "GwAIuRGhnAuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listar procesos"
      ],
      "metadata": {
        "id": "IhHGrQ5nW7OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -eaf"
      ],
      "metadata": {
        "id": "fuuLLHN7nM0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se mata el proceso\n",
        "```\n",
        "# Reemplazar el numero con el proceso elegido\n",
        "```"
      ],
      "metadata": {
        "id": "8lMTxRT2W-Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 2406"
      ],
      "metadata": {
        "id": "NIiD2sHznROo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "O4pQ_-OxUcTx",
        "iiKTjaKrVNQ5",
        "7hw-cywfVW7O",
        "dTLVBFBWViIl",
        "jbgrm0xfWEyr",
        "XkKRC932W0RT"
      ],
      "authorship_tag": "ABX9TyNpgf37w6Lf8665cfXxMZwf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}